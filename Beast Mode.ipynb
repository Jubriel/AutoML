{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Ademonla:\n",
    "#     def __init__(self, scoring_function = 'f1', n_iter = 50):\n",
    "#         self.scoring_function = scoring_function\n",
    "#         self.n_iter = n_iter\n",
    "    \n",
    "#     def fit(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Full_course(Trainfile, Testfile, target, ptype, metric):\n",
    "    # importing all required libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    # from sklearn.model_selection import RandomizedSearchCV\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # Loading the dataframe\n",
    "    Trainframe = pd.read_csv(Trainfile,encoding='cp1252')\n",
    "    Testframe = pd.read_csv(Testfile,encoding='cp1252')\n",
    "\n",
    "    # Removes columns with more than 70% missing values\n",
    "    crazy = [i for i in Trainframe.columns if Trainframe[i].isna().sum() >= 0.7*len(Trainframe[i])]\n",
    "    Trainframe = Trainframe.drop(crazy, axis=1)\n",
    "    Testframe = Testframe.drop(crazy, axis=1)\n",
    "\n",
    "   # Sorting based on datatypes\n",
    "    date = [d for d in Trainframe.columns if 'Date' in d]\n",
    "    cat = [n for n in Trainframe.columns if Trainframe[n].dtype == object and n not in date]\n",
    "    # cat = []\n",
    "    # for n in Trainframe.columns:\n",
    "    #     if Trainframe[n].dtype == object or Trainframe[n].dtype is 'category' or Trainframe[n].dtype == bool and n not in date:\n",
    "    #         cat.append(n)\n",
    "    num = [p for p in Trainframe.columns if p not in cat]\n",
    "\n",
    "    # filling up missing values\n",
    "    imp = KNNImputer(n_neighbors=5)\n",
    "    missing_num = [i for i in num if Trainframe[i].isna().sum() != 0]\n",
    "    if missing_num != []:\n",
    "        Trainframe[missing_num] = imp.fit_transform(Trainframe[missing_num])\n",
    "        Testframe[missing_num] = imp.transform(Testframe[missing_num])\n",
    "\n",
    "    obj = SimpleImputer(missing_values = np.nan, strategy='most_frequent')\n",
    "    missing_cat = [i for i in cat if Trainframe[i].isna().sum() != 0]\n",
    "    if missing_cat != []:\n",
    "        Trainframe[missing_cat] = obj.fit_transform(Trainframe[missing_cat])\n",
    "        Testframe[missing_cat] = obj.transform(Testframe[missing_cat])\n",
    "\n",
    "        Trainframe[cat] = Trainframe[cat].astype('category')\n",
    "        Testframe[cat] = Testframe[cat].astype('category')\n",
    "\n",
    "    # Dealing with date features\n",
    "    if date != []:\n",
    "        Trainframe[date] = obj.fit_transform(Trainframe[date])\n",
    "        Testframe[date] = obj.transform(Testframe[date])\n",
    "        for i in date:\n",
    "            Trainframe[i] = pd.to_datetime(Trainframe[i]).astype('category')\n",
    "            Trainframe[f'{i}_months'] = Trainframe[i].dt.month.astype('category')\n",
    "            Trainframe[f'{i}_days'] = Trainframe[i].dt.day.astype('category')\n",
    "            Trainframe[f'{i}_years'] = Trainframe[i].dt.year.astype('category')\n",
    "\n",
    "            Testframe[i] = pd.to_datetime(Testframe[i]).astype('category')\n",
    "            Testframe[f'{i}_months'] = Testframe[i].dt.month.astype('category')\n",
    "            Testframe[f'{i}_days'] = Testframe[i].dt.day.astype('category')\n",
    "            Testframe[f'{i}_years'] = Testframe[i].dt.year.astype('category')\n",
    "\n",
    "        Trainframe = Trainframe.drop(date, axis = 1)\n",
    "        Testframe = Testframe.drop(date, axis = 1)\n",
    "    \n",
    "    iid = Testframe['ID']\n",
    "\n",
    "    # Removing columns with high variance\n",
    "    uniq = [i for i in cat if Trainframe[i].nunique() >= 0.7 * Trainframe.shape[0]]\n",
    "    Trainframe = Trainframe.drop(uniq, axis=1)\n",
    "    Testframe = Testframe.drop(uniq, axis=1)\n",
    "\n",
    "    Testframe1 = Testframe.copy()\n",
    "\n",
    "    X = Trainframe.drop(target, axis =1)\n",
    "    y = Trainframe[target]\n",
    "\n",
    "    # Train and test split\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 101)\n",
    "    trainX, testX, trainy, testy = train_test_split(X, y, test_size = 0.2, random_state = 101)\n",
    "    t = [i for i in cat if i not in uniq]\n",
    "\n",
    "\n",
    "    #Selecting Classification or Regression---TPOT\n",
    "    if ptype == 'class':\n",
    "      # Encoding\n",
    "        encode = ce.BinaryEncoder(t)\n",
    "        train_X = encode.fit_transform(train_X)\n",
    "        test_X = encode.transform(test_X)\n",
    "        Testframe = encode.transform(Testframe)\n",
    "    \n",
    "      # Resampling\n",
    "        oversample = SMOTE()\n",
    "        train_X, train_y = oversample.fit_resample(train_X, train_y)\n",
    "        #test_X, test_y = oversample.fit_resample(test_X, test_y)\n",
    "\n",
    "        # Feature Selection \n",
    "        if len(train_X.columns) > 20:\n",
    "            dec = PCA(0.98)\n",
    "            train_X = dec.fit_transform(train_X)\n",
    "            test_X = dec.transform(test_X)\n",
    "            Testframe = dec.transform(Testframe)\n",
    "\n",
    "      # Standardization\n",
    "        sc = RobustScaler()\n",
    "        train_X = sc.fit_transform(train_X)\n",
    "        test_X = sc.transform(test_X)\n",
    "        Testframe = sc.transform(Testframe)\n",
    "        \n",
    "      # Using the TPOT AutoML\n",
    "        tpot_class = TPOTClassifier(max_time_mins=10,scoring= metric)\n",
    "        tpot_class.fit(train_X, train_y) #fit the model on the Train\n",
    "\n",
    "        ypred = tpot_class.predict(test_X)\n",
    "        # Predict the Testfile\n",
    "        ypred1 = tpot_class.predict(Testframe)\n",
    "        # Check the Scores\n",
    "        sol = tpot_class.score(train_X, train_y)\n",
    "        sol1 = tpot_class.score(test_X, test_y)        \n",
    "\n",
    "    elif ptype == 'reg':\n",
    "        # Encoding the categorical Variables\n",
    "        encode = ce.TargetEncoder(t, smoothing=8, min_samples_leaf=5)\n",
    "        train_X = encode.fit_transform(train_X, train_y)\n",
    "        test_X = encode.transform(test_X)\n",
    "        Testframe = encode.transform(Testframe)\n",
    "\n",
    "      # Standardization\n",
    "        sc = RobustScaler()\n",
    "        train_X = sc.fit_transform(train_X)\n",
    "        test_X = sc.transform(test_X)\n",
    "        Testframe = sc.transform(Testframe)\n",
    "\n",
    "        # Using the TPOT AutoML\n",
    "        tpot_reg = TPOTRegressor(max_time_mins=10,scoring= metric)\n",
    "        tpot_reg.fit(train_X, train_y) # fit the model in the Train set\n",
    "\n",
    "        ypred = tpot_reg.predict(test_X)\n",
    "        # Predict the Test file\n",
    "        ypred1 = tpot_reg.predict(Testframe)\n",
    "        # Check the Scores\n",
    "        sol = tpot_reg.score(train_X, train_y)    \n",
    "        sol1 = tpot_reg.score(test_X, test_y)\n",
    "    \n",
    "    # Outcomes\n",
    "    alg = pd.DataFrame({\"ID\":iid,f'{target}':ypred1})\n",
    "    Scores = f'TPOT: The Train score is {sol}, The Test score is {sol1}'\n",
    "    print(Scores)\n",
    "\n",
    "\n",
    "\n",
    "    #Selecting Classification or Regression--- Ensemble\n",
    "    if ptype == 'class':\n",
    "        # category encoding \n",
    "        encode = ce.BinaryEncoder(t)\n",
    "        trainX = encode.fit_transform(trainX)\n",
    "        testX = encode.transform(testX)\n",
    "        Testframe1 = encode.transform(Testframe1)\n",
    "\n",
    "        # Resampling\n",
    "        oversample = SMOTE()\n",
    "        trainX, trainy = oversample.fit_resample(trainX, trainy)\n",
    "        #testX, testy = oversample.fit_resample(testX, testy)\n",
    "        \n",
    "        # Feature Selection \n",
    "        if len(trainX.columns) > 20:\n",
    "            dec = PCA(0.98)\n",
    "            trainX = dec.fit_transform(trainX)\n",
    "            testX = dec.transform(testX)\n",
    "            Testframe1 = dec.transform(Testframe1)\n",
    "\n",
    "        # Standardization\n",
    "        sc = RobustScaler()\n",
    "        trainX = sc.fit_transform(trainX)\n",
    "        testX = sc.transform(testX)\n",
    "        Testframe1 = sc.transform(Testframe1)\n",
    "\n",
    "        # Modeling\n",
    "        # Finding the BaseLine perfomance of the various models\n",
    "        models = []\n",
    "\n",
    "        # Adding algorthms\n",
    "        models.append(('cat', CatBoostClassifier(verbose=0)))\n",
    "        models.append(('mlp', MLPClassifier(verbose=0)))\n",
    "        models.append(('lgr', LogisticRegression(verbose=0)))\n",
    "        models.append(('svc', SVC(probability=True)))\n",
    "        models.append(('knc', KNeighborsClassifier()))\n",
    "        models.append(('lgbm', LGBMClassifier()))\n",
    "        models.append(('sgd', SGDClassifier()))\n",
    "        models.append(('ext', ExtraTreesClassifier()))\n",
    "        models.append(('rfc', RandomForestClassifier()))\n",
    "        models.append(('gbc', GradientBoostingClassifier()))\n",
    "        models.append(('xgb', XGBClassifier()))\n",
    "\n",
    "        # evaluate -cross validation- each model in turn\n",
    "        results = []\n",
    "        mods = []\n",
    "        names = []\n",
    "        scoring =['roc_auc', metric]\n",
    "        for name, model in models:\n",
    "            kfold = StratifiedKFold(n_splits=7, shuffle= True)\n",
    "            cv_results = cross_validate(model, trainX, trainy, cv=kfold, scoring=scoring, return_train_score=True)\n",
    "            results.append([cv_results['test_roc_auc'].mean(), cv_results[f'test_{metric}'].mean()])\n",
    "            mods.append(model)\n",
    "            names.append(name)\n",
    "        alg = pd.DataFrame({'models': models, 'roc_auc':[m[0] for m in results] , metric: [m[1] for m in results]})\n",
    "        # Ensemble\n",
    "        vtc = VotingClassifier([n for n in alg.sort_values(metric, ascending=False)['models'][:2]], weights =[i for i in alg[metric].nlargest(2)], voting='soft')\n",
    "        vtc.fit(trainX, trainy) # fit the model in the Train set\n",
    "        # if vote < alg.metric.max():\n",
    "        #   alg.metric.max()['models']\n",
    "        ypred = vtc.predict(testX)\n",
    "        \n",
    "        # Predict the Test file\n",
    "        ypred1 = vtc.predict(Testframe1)\n",
    "        ypredp1 = vtc.predict_proba(Testframe1)\n",
    "        \n",
    "        # Check the Scores\n",
    "        sol = vtc.score(trainX, trainy)    \n",
    "        sol1 = vtc.score(testX, testy)\n",
    "\n",
    "    elif ptype == 'reg':\n",
    "        # category encoding \n",
    "        encode = ce.TargetEncoder(cols=t, smoothing=8, min_samples_leaf=5)\n",
    "        trainX = encode.fit_transform(trainX, trainy)\n",
    "        testX = encode.transform(testX)\n",
    "        Testframe1 = encode.transform(Testframe1)\n",
    "\n",
    "        # Feature Selection \n",
    "        dec = PCA(0.98)\n",
    "        trainX = dec.fit_transform(trainX)\n",
    "        testX = dec.transform(testX)\n",
    "        Testframe1 = dec.transform(Testframe1)\n",
    "\n",
    "        # Standardization\n",
    "        sc = RobustScaler()\n",
    "        trainX = sc.fit_transform(trainX)\n",
    "        testX = sc.transform(testX)\n",
    "        Testframe1 = sc.transform(Testframe1)\n",
    "\n",
    "        # Modeling\n",
    "        # Finding the BaseLine perfomance of the various models\n",
    "\n",
    "        # Prepare models\n",
    "        models = []\n",
    "\n",
    "        # Adding algorthms\n",
    "        models.append(('cat', CatBoostRegressor(verbose=0)))\n",
    "        models.append(('mlp', MLPRegressor(verbose=0)))\n",
    "        models.append(('lgr', LinearRegression()))\n",
    "        models.append(('lss', Lasso()))\n",
    "        models.append(('eln', ElasticNet()))\n",
    "        models.append(('svc', SVR()))\n",
    "        models.append(('knc', KNeighborsRegressor()))\n",
    "        models.append(('sgd', SGDRegressor()))\n",
    "        models.append(('lgbm', LGBMRegressor()))\n",
    "        models.append(('ext', ExtraTreesRegressor()))\n",
    "        models.append(('rfc', RandomForestRegressor()))\n",
    "        models.append(('gbc', GradientBoostingRegressor()))\n",
    "        models.append(('xgb', XGBRegressor()))\n",
    "       \n",
    "        # evaluate -cross validation- each model in turn\n",
    "        results = []\n",
    "        names = []\n",
    "        scoring =['neg_root_mean_squared_error', metric]\n",
    "        for name, model in models:\n",
    "            kfold = KFold(n_splits=7, shuffle= True)\n",
    "            cv_results = cross_validate(model, trainX, trainy, cv=kfold, scoring=scoring, return_train_score=True)\n",
    "            results.append([-cv_results['test_neg_root_mean_squared_error'].mean(),cv_results[f'test_{metric}'].mean()])\n",
    "            names.append(name)\n",
    "        alg = pd.DataFrame({'models': models, 'RMSE':[m[0] for m in results], metric:[m[1] for m in results]})\n",
    "        # Ensemble\n",
    "        vtr = VotingRegressor([n for n in alg.sort_values('RMSE', ascending=True)['models']][:2], weights =[i for i in alg[metric].nsmallest(2)], voting = 'hard')\n",
    "        vtr.fit(trainX, trainy) # fit the model in the Train set\n",
    "\n",
    "        ypred = vtr.predict(test_X)\n",
    "        # Predict the Test file\n",
    "        ypred1 = vt.predict(Testframe1)\n",
    "        # Check the Scores\n",
    "        sol = vtr.score(trainX, trainy)    \n",
    "        sol1 = vtr.score(testX, testy)\n",
    "\n",
    "    # Outcomes\n",
    "    alg = pd.DataFrame({'ID':iid ,f'{target}':ypred1})\n",
    "    Scores = f'Ensemble: The Train score is {sol}, The Test score is {sol1}'\n",
    "    print(Scores)\n",
    "\n",
    "\n",
    "    return alg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Policy Start Date', 'Policy End Date', 'First Transaction Date']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Policy Start Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2894\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2895\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Policy Start Date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-be993342d5b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mADemonLa\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/iDAFAdmin/Documents/Projects/Zindi/Autoland Insurance Claim/Train.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'C:/Users/iDAFAdmin/Documents/Projects/Zindi/Autoland Insurance Claim/Test.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'target'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'class'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'f1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-fce126b5f0c0>\u001b[0m in \u001b[0;36mADemonLa\u001b[1;34m(Trainfile, Testfile, target, ptype, metric)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;31m# Removing columns with high variance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0muniq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcat\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mTrainframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.7\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mTrainframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[0mTrainframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[0mTestframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTestframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-fce126b5f0c0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;31m# Removing columns with high variance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0muniq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcat\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mTrainframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.7\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mTrainframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[0mTrainframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrainframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[0mTestframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTestframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2904\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2895\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Policy Start Date'"
     ]
    }
   ],
   "source": [
    "Demon('C:/Users/iDAFAdmin/Documents/Projects/Zindi/Autoland Insurance Claim/Train.csv', 'C:/Users/iDAFAdmin/Documents/Projects/Zindi/Autoland Insurance Claim/Test.csv', 'target', 'class', 'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "name": "python3710jvsc74a57bd034557c4129972600f46ab73f6d60d31e0ec191e7d378fd14c15cd532233346ff"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
